# Step 1: Load the data: Use the following package from sklearn.datasets 
    # import load_digits. Each instance is 8x8 image 
    # (64 pixels/features) and there are 1700+ instances.
import pandas as pd
from sklearn.datasets import load_digits
#assighn and load the digiti data sets

digits=load_digits(as_frame=True)
digits

### understand the data more 
print(digits.keys())
print(digits.DESCR)
print(digits.images.shape)
print(digits.data.shape)


# attributes: pixels 
# classifiers: numbers 0-9 
# hypothesis: X pixels = number classifier 



# Step 2: Perform 5-fold cross validation. 
    # Use from sklearn.tree import DecisionTreeClassifier, 
    # from sklearn.model_selection import cross_val_score 
    # to perform cross validation. Vary the max_depth parameter 
    # in DecisionTreeClassifier from 1 to 10. 
        ##How does the averagecross validation error change for each of these depths? 
    # You can draw a table (or plot a graph using  import matplotlib.pyplot as plt). 
    
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier

x = digits.data # attributes: pixels 
y = digits.target # classifiers: numbers 0-9 


# depth makes the tree have different nodes and more or less complex
#TRAINING NOTES
# min_sample_split = how many you need to split 
# max_depth = 
# min_sample_leaf = 

### Max Depth 1 ###

clf = DecisionTreeClassifier(max_depth=1,criterion="entropy").fit(x, y)
md1 = cross_val_score(clf,x,y,cv=5) 
md1

### Max Depth 2 ###

clf = DecisionTreeClassifier(max_depth=2,criterion="entropy").fit(x, y)
md2 = cross_val_score(clf,x,y,cv=5) 
md2

### Max Depth 3 ###

clf = DecisionTreeClassifier(max_depth=3,criterion="entropy").fit(x, y)
md3 = cross_val_score(clf,x,y,cv=5) 
md3

### Max Depth 4 ###

clf = DecisionTreeClassifier(max_depth=4,criterion="entropy").fit(x, y)
md4 = cross_val_score(clf,x,y,cv=5) 
md4

### Max Depth 5 ###

clf = DecisionTreeClassifier(max_depth=5,criterion="entropy").fit(x, y)
md5 = cross_val_score(clf,x,y,cv=5) 
md5

### Max Depth 6 ###

clf = DecisionTreeClassifier(max_depth=6,criterion="entropy").fit(x, y)
md6 = cross_val_score(clf,x,y,cv=5) 
md6

### Max Depth 7 ###

clf = DecisionTreeClassifier(max_depth=7,criterion="entropy").fit(x, y)
md7 = cross_val_score(clf,x,y,cv=5) 
md7

### Max Depth 8 ###

clf = DecisionTreeClassifier(max_depth=8,criterion="entropy").fit(x, y)
md8 = cross_val_score(clf,x,y,cv=5) 
md8

### Max Depth 9 ###

clf = DecisionTreeClassifier(max_depth=9,criterion="entropy").fit(x, y)
md9 = cross_val_score(clf,x,y,cv=5) 
md9


def display_md9(md9):
    print(md9)


# data things 
import numpy as np

table = np.array([md1, md2, md3, md4, md5, md6, md7, md8, md9])

print(table)



#Step 3: Analyze the feature importances 
    #(for the best max_depth setting) 
    #(you an refer to the video and demo code)@ 9:37. 
    # Which were the 10 most important pixels for the classifier. 
    # Were the important pixels meaningful to you? 
    

### Max Depth 1 ###

clf = DecisionTreeClassifier(max_depth=1,criterion="entropy").fit(x, y)
md1_f = clf.feature_importances_

### Max Depth 2 ###

clf = DecisionTreeClassifier(max_depth=2,criterion="entropy").fit(x, y)
md2_f = clf.feature_importances_


### Max Depth 3 ###

clf = DecisionTreeClassifier(max_depth=3,criterion="entropy").fit(x, y)
md3_f = clf.feature_importances_

### Max Depth 4 ###

clf = DecisionTreeClassifier(max_depth=4,criterion="entropy").fit(x, y)
md4_f = clf.feature_importances_

### Max Depth 5 ###

clf = DecisionTreeClassifier(max_depth=5,criterion="entropy").fit(x, y)
md5_f = clf.feature_importances_

### Max Depth 6 ###

clf = DecisionTreeClassifier(max_depth=6,criterion="entropy").fit(x, y)
md6_f = clf.feature_importances_

### Max Depth 7 ###

clf = DecisionTreeClassifier(max_depth=7,criterion="entropy").fit(x, y)
md7_f = clf.feature_importances_

### Max Depth 8 ###

clf = DecisionTreeClassifier(max_depth=8,criterion="entropy").fit(x, y)
md8_f = clf.feature_importances_

### Max Depth 9 ###

clf = DecisionTreeClassifier(max_depth=9,criterion="entropy").fit(x, y)
md9_f = clf.feature_importances_

table_f = np.array([md1_f, md2_f, md3_f, md4_f, md5_f, md6_f, md7_f, md8_f, md9_f])

print(md3_f)




from sklearn.tree import plot_tree
from matplotlib import pyplot as plt

plt.figure()
clf = DecisionTreeClassifier(max_depth=1,criterion="entropy").fit(x, y)
plot_tree(clf,filled=True)
plt.title('decision tree')
plt.show()


###Suggest some steps you could take that could improve the performance of the classifier.###

#To help improve perfromance I would do a 10 K-fold beacuse the lower the k fold the higher risk of bias

#Find more people to share how they write numbers to add to the data set, it is a small sample based on the amout of classifiers. 
