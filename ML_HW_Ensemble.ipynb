{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Isdabelle Kernell\nMarch 18, 2022\nAssignment-5-Ensemble\nCOMP 7745","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_iris\nfrom sklearn.ensemble import AdaBoostClassifier #boost learning sequentially(not independent) then combined \nfrom sklearn.ensemble import BaggingClassifier #bagging class learning independently then combined \nfrom sklearn.model_selection import train_test_split  \nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\n\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-03-19T04:19:24.996984Z","iopub.execute_input":"2022-03-19T04:19:24.997741Z","iopub.status.idle":"2022-03-19T04:19:25.003023Z","shell.execute_reply.started":"2022-03-19T04:19:24.997700Z","shell.execute_reply":"2022-03-19T04:19:25.002170Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"In this assignment, we will compare bagging and boosting methods. \n\nUse the dataset \"**sklearn.datasets.load_iris**\".\n\nThe bagging classifier is defined in \"sklearn.ensemble.BaggingClassifier\" and the boosting classifier is defined in \"sklearn.ensemble.AdaBoostClassifier\" (use the **SAMME discrete boosting algorithm** since this is what we discussed in the slides)\n\nFor both bagging and boosting use DecisionTreeClassifier as the base classifier with **max_depth equal to 1.**\n\nCompare the accuracy on the full dataset (training accuracy) as well as 5-fold cross validation accuracy (generalization accuracy) for the 2 classifiers as we vary the number of estimators (n_estimators parameter which controls how many classifiers are added in the ensemble) between 1 and 10. \n\n\nDid the bias reduce (training accuracy becomes larger) when we increased the number of estimators in both bagging and boosting? What was the effect of adding estimators on 5-fold cross validation accuracy?\nSubmit a Jupyter notebook for this assignment.","metadata":{}},{"cell_type":"code","source":"data = load_iris()\n\nX = data.data   #Features\ny = data.target  #Target variable\n\nprint(X)\nprint(y)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-19T04:19:25.005048Z","iopub.execute_input":"2022-03-19T04:19:25.005855Z","iopub.status.idle":"2022-03-19T04:19:25.030795Z","shell.execute_reply.started":"2022-03-19T04:19:25.005819Z","shell.execute_reply":"2022-03-19T04:19:25.029984Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T04:19:25.032408Z","iopub.execute_input":"2022-03-19T04:19:25.032720Z","iopub.status.idle":"2022-03-19T04:19:25.038691Z","shell.execute_reply.started":"2022-03-19T04:19:25.032677Z","shell.execute_reply":"2022-03-19T04:19:25.037850Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"######n_estimators = 1######\nclf_boost1 =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=1,algorithm=\"SAMME\")\n\nclf_boost1.fit(X,y)\nprint(clf_boost1.estimator_errors_)\nprint(clf_boost1.estimator_weights_)\n\n# 5-fold cross validation accuracy\nboost_fold1 = cross_val_score(clf_boost1,X,y,cv=5) \nprint(boost_fold1)\n\n#### bagging 1 ####\nmodel_bagging1 = BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=1)\n\nmodel_bagging1.fit(X_train, y_train)\nbaggg_score1 = model_bagging1.score(X_train, y_train)\nprint(baggg_score1)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-19T04:22:48.054365Z","iopub.execute_input":"2022-03-19T04:22:48.055209Z","iopub.status.idle":"2022-03-19T04:22:48.090453Z","shell.execute_reply.started":"2022-03-19T04:22:48.055164Z","shell.execute_reply":"2022-03-19T04:22:48.089637Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"######n_estimators = 2 ######\nclf_boost2 =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=2,algorithm=\"SAMME\")\n\nclf_boost2.fit(X,y)\nprint(clf_boost2.estimator_errors_)\nprint(clf_boost2.estimator_weights_)\n\n# 5-fold cross validation accuracy\nboost_fold2 = cross_val_score(clf_boost2,X,y,cv=5) \nprint(boost_fold2)\n\n#### bagging 2 ####\nmodel_bagging2 = BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=2)\n\nmodel_bagging2.fit(X_train, y_train)\nbaggg_score2 = model_bagging2.score(X_train, y_train)\nprint(baggg_score2)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T04:19:25.081945Z","iopub.execute_input":"2022-03-19T04:19:25.082280Z","iopub.status.idle":"2022-03-19T04:19:25.124584Z","shell.execute_reply.started":"2022-03-19T04:19:25.082249Z","shell.execute_reply":"2022-03-19T04:19:25.123745Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"######n_estimators = 3 ######\nclf_boost3 =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=3,algorithm=\"SAMME\")\n\nclf_boost3.fit(X,y)\nprint(clf_boost3.estimator_errors_)\nprint(clf_boost3.estimator_weights_)\n\n# 5-fold cross validation accuracy\nboost_fold3 = cross_val_score(clf_boost3,X,y,cv=5) \nprint(boost_fold3)\n\n#### bagging 3 ####\nmodel_bagging3 = BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=3)\n\nmodel_bagging3.fit(X_train, y_train)\nbaggg_score3 = model_bagging3.score(X_train, y_train)\nprint(baggg_score3)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T04:19:25.125951Z","iopub.execute_input":"2022-03-19T04:19:25.126600Z","iopub.status.idle":"2022-03-19T04:19:25.181725Z","shell.execute_reply.started":"2022-03-19T04:19:25.126557Z","shell.execute_reply":"2022-03-19T04:19:25.180878Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"######n_estimators = 4 ######\nclf_boost4 =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=4,algorithm=\"SAMME\")\n\nclf_boost4.fit(X,y)\nprint(clf_boost4.estimator_errors_)\nprint(clf_boost4.estimator_weights_)\n\n# 5-fold cross validation accuracy\nboost_fold4 = cross_val_score(clf_boost4,X,y,cv=5) \nprint(boost_fold4)\n\n#### bagging 4 ####\nmodel_bagging4 = BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=4)\n\nmodel_bagging4.fit(X_train, y_train)\nbaggg_score4 = model_bagging4.score(X_train, y_train)\nprint(baggg_score4)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-19T04:19:25.182933Z","iopub.execute_input":"2022-03-19T04:19:25.183153Z","iopub.status.idle":"2022-03-19T04:19:25.248291Z","shell.execute_reply.started":"2022-03-19T04:19:25.183125Z","shell.execute_reply":"2022-03-19T04:19:25.247388Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"######n_estimators = 5 ######\nclf_boost5 =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=5,algorithm=\"SAMME\")\n\nclf_boost5.fit(X,y)\nprint(clf_boost5.estimator_errors_)\nprint(clf_boost5.estimator_weights_)\n\n# 5-fold cross validation accuracy\nboost_fold5 = cross_val_score(clf_boost5,X,y,cv=5) \nprint(boost_fold5)\n\n#### bagging 5 ####\nmodel_bagging5 = BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=5)\n\nmodel_bagging5.fit(X_train, y_train)\nbaggg_score5 = model_bagging5.score(X_train, y_train)\nprint(baggg_score5)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T04:19:25.249759Z","iopub.execute_input":"2022-03-19T04:19:25.250251Z","iopub.status.idle":"2022-03-19T04:19:25.325713Z","shell.execute_reply.started":"2022-03-19T04:19:25.250210Z","shell.execute_reply":"2022-03-19T04:19:25.324900Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"######n_estimators = 6 ######\nclf_boost6 =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=6,algorithm=\"SAMME\")\n\nclf_boost6.fit(X,y)\nprint(clf_boost6.estimator_errors_)\nprint(clf_boost6.estimator_weights_)\n\n# 5-fold cross validation accuracy\nboost_fold6 = cross_val_score(clf_boost6,X,y,cv=5) \nprint(boost_fold6)\n\n#### bagging 6 ####\n\nmodel_bagging6 = BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=6)\n\nmodel_bagging6.fit(X_train, y_train)\nbaggg_score6 = model_bagging6.score(X_train, y_train)\nprint(baggg_score6)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-19T04:19:25.327234Z","iopub.execute_input":"2022-03-19T04:19:25.327675Z","iopub.status.idle":"2022-03-19T04:19:25.412643Z","shell.execute_reply.started":"2022-03-19T04:19:25.327633Z","shell.execute_reply":"2022-03-19T04:19:25.411841Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"######n_estimators = 7 ######\nclf_boost7 =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=7,algorithm=\"SAMME\")\n\nclf_boost7.fit(X,y)\nprint(clf_boost7.estimator_errors_)\nprint(clf_boost7.estimator_weights_)\n\n# 5-fold cross validation accuracy\nboost_fold7 = cross_val_score(clf_boost7,X,y,cv=5) \nprint(boost_fold7)\n\n#### bagging 7 ####\n\nmodel_bagging7 = BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=7)\n\nmodel_bagging7.fit(X_train, y_train)\nbaggg_score7 = model_bagging7.score(X_train, y_train)\nprint(baggg_score7)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T04:19:25.413923Z","iopub.execute_input":"2022-03-19T04:19:25.414140Z","iopub.status.idle":"2022-03-19T04:19:25.511110Z","shell.execute_reply.started":"2022-03-19T04:19:25.414113Z","shell.execute_reply":"2022-03-19T04:19:25.510529Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"######n_estimators = 8 ######\nclf_boost8 =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=8,algorithm=\"SAMME\")\n\nclf_boost8.fit(X,y)\nprint(clf_boost8.estimator_errors_)\nprint(clf_boost8.estimator_weights_)\n\n# 5-fold cross validation accuracy\nboost_fold8 = cross_val_score(clf_boost8,X,y,cv=5) \nprint(boost_fold8)\n\n#### bagging 8 ####\n\nmodel_bagging8 = BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=8)\n\nmodel_bagging8.fit(X_train, y_train)\nbaggg_score8 = model_bagging8.score(X_train, y_train)\nprint(baggg_score8)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T04:19:25.512186Z","iopub.execute_input":"2022-03-19T04:19:25.512382Z","iopub.status.idle":"2022-03-19T04:19:25.627429Z","shell.execute_reply.started":"2022-03-19T04:19:25.512357Z","shell.execute_reply":"2022-03-19T04:19:25.626577Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"######n_estimators = 9 ######\nclf_boost9 =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=9,algorithm=\"SAMME\")\n\nclf_boost9.fit(X,y)\nprint(clf_boost9.estimator_errors_)\nprint(clf_boost9.estimator_weights_)\n\n# 5-fold cross validation accuracy\nboost_fold9 = cross_val_score(clf_boost9,X,y,cv=5) \nprint(boost_fold9)\n\n#### bagging 9 ####\n\nmodel_bagging9 = BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=9)\n\nmodel_bagging9.fit(X_train, y_train)\nbaggg_score9 = model_bagging9.score(X_train, y_train)\nprint(baggg_score9)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T04:19:25.629165Z","iopub.execute_input":"2022-03-19T04:19:25.629476Z","iopub.status.idle":"2022-03-19T04:19:25.749573Z","shell.execute_reply.started":"2022-03-19T04:19:25.629436Z","shell.execute_reply":"2022-03-19T04:19:25.748716Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"######n_estimators = 10 ######\nclf_boost10 =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=10,algorithm=\"SAMME\")\n\nclf_boost10.fit(X,y)\n#print(clf_boost10.estimator_errors_)\n#print(clf_boost10.estimator_weights_)\n\n# 5-fold cross validation accuracy\nboost_fold10 = cross_val_score(clf_boost10,X,y,cv=5) \nprint(boost_fold10)\n\n#### bagging 10 ####\n\nmodel_bagging10 = BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=10)\n\nmodel_bagging10.fit(X_train, y_train)\nbaggg_score10 = model_bagging10.score(X_train, y_train)\nprint(baggg_score10)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T04:19:47.087950Z","iopub.execute_input":"2022-03-19T04:19:47.088217Z","iopub.status.idle":"2022-03-19T04:19:47.218573Z","shell.execute_reply.started":"2022-03-19T04:19:47.088189Z","shell.execute_reply":"2022-03-19T04:19:47.217737Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Did the bias reduce (training accuracy becomes larger) when we increased the number of estimators in both bagging and boosting? \nThe higher the n_estimators the more accurate the data \n\n# What was the effect of adding estimators on 5-fold cross validation accuracy? \n5 fold was higher and over fitted when compared to the boosting *","metadata":{}},{"cell_type":"code","source":"######n_estimators = 1 LAB video notes ######\nclf_boost1 =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=1,algorithm=\"SAMME\")\nclf_boost1.fit(X,y)\nprint(clf_boost1.estimator_errors_) # after adding what would the error be\nprint(clf_boost1.estimator_weights_) #smaller the error> the larger the weight\n\n# 5-fold cross validation accuracy\nboost_fold1 = cross_val_score(clf_boost1,X,y,cv=5) \n#print(boost_fold1)\n\n#table = np.array([clf_boost1.estimator_errors_, clf_boost10.estimator_weights_])\n\n#print(table)\n#print(boost_fold10)\n\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-19T04:19:25.886130Z","iopub.execute_input":"2022-03-19T04:19:25.886377Z","iopub.status.idle":"2022-03-19T04:19:25.915027Z","shell.execute_reply.started":"2022-03-19T04:19:25.886343Z","shell.execute_reply":"2022-03-19T04:19:25.914161Z"},"trusted":true},"execution_count":19,"outputs":[]}]}
