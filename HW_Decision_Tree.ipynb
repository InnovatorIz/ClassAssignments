{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Load the data: Use the following package from sklearn.datasets \n    # import load_digits. Each instance is 8x8 image \n    # (64 pixels/features) and there are 1700+ instances.\nimport pandas as pd\nfrom sklearn.datasets import load_digits\n#assighn and load the digiti data sets\n\ndigits=load_digits(as_frame=True)\ndigits\n\n### understand the data more \nprint(digits.keys())\nprint(digits.DESCR)\nprint(digits.images.shape)\nprint(digits.data.shape)\n\n\n# attributes: pixels \n# classifiers: numbers 0-9 \n# hypothesis: X pixels = number classifier \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-12T03:27:42.635062Z","iopub.execute_input":"2022-02-12T03:27:42.635315Z","iopub.status.idle":"2022-02-12T03:27:44.029202Z","shell.execute_reply.started":"2022-02-12T03:27:42.635239Z","shell.execute_reply":"2022-02-12T03:27:44.028305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 2: Perform 5-fold cross validation. \n    # Use from sklearn.tree import DecisionTreeClassifier, \n    # from sklearn.model_selection import cross_val_score \n    # to perform cross validation. Vary the max_depth parameter \n    # in DecisionTreeClassifier from 1 to 10. \n        ##How does the averagecross validation error change for each of these depths? \n    # You can draw a table (or plot a graph using  import matplotlib.pyplot as plt). \n    \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nx = digits.data # attributes: pixels \ny = digits.target # classifiers: numbers 0-9 \n\n\n# depth makes the tree have different nodes and more or less complex\n#TRAINING NOTES\n# min_sample_split = how many you need to split \n# max_depth = \n# min_sample_leaf = \n\n### Max Depth 1 ###\n\nclf = DecisionTreeClassifier(max_depth=1,criterion=\"entropy\").fit(x, y)\nmd1 = cross_val_score(clf,x,y,cv=5) \nmd1\n\n### Max Depth 2 ###\n\nclf = DecisionTreeClassifier(max_depth=2,criterion=\"entropy\").fit(x, y)\nmd2 = cross_val_score(clf,x,y,cv=5) \nmd2\n\n### Max Depth 3 ###\n\nclf = DecisionTreeClassifier(max_depth=3,criterion=\"entropy\").fit(x, y)\nmd3 = cross_val_score(clf,x,y,cv=5) \nmd3\n\n### Max Depth 4 ###\n\nclf = DecisionTreeClassifier(max_depth=4,criterion=\"entropy\").fit(x, y)\nmd4 = cross_val_score(clf,x,y,cv=5) \nmd4\n\n### Max Depth 5 ###\n\nclf = DecisionTreeClassifier(max_depth=5,criterion=\"entropy\").fit(x, y)\nmd5 = cross_val_score(clf,x,y,cv=5) \nmd5\n\n### Max Depth 6 ###\n\nclf = DecisionTreeClassifier(max_depth=6,criterion=\"entropy\").fit(x, y)\nmd6 = cross_val_score(clf,x,y,cv=5) \nmd6\n\n### Max Depth 7 ###\n\nclf = DecisionTreeClassifier(max_depth=7,criterion=\"entropy\").fit(x, y)\nmd7 = cross_val_score(clf,x,y,cv=5) \nmd7\n\n### Max Depth 8 ###\n\nclf = DecisionTreeClassifier(max_depth=8,criterion=\"entropy\").fit(x, y)\nmd8 = cross_val_score(clf,x,y,cv=5) \nmd8\n\n### Max Depth 9 ###\n\nclf = DecisionTreeClassifier(max_depth=9,criterion=\"entropy\").fit(x, y)\nmd9 = cross_val_score(clf,x,y,cv=5) \nmd9\n\n\ndef display_md9(md9):\n    print(md9)\n\n\n# data things \nimport numpy as np\n\ntable = np.array([md1, md2, md3, md4, md5, md6, md7, md8, md9])\n\nprint(table)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-12T03:27:58.609626Z","iopub.execute_input":"2022-02-12T03:27:58.61067Z","iopub.status.idle":"2022-02-12T03:27:59.869865Z","shell.execute_reply.started":"2022-02-12T03:27:58.610594Z","shell.execute_reply":"2022-02-12T03:27:59.869235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#Step 3: Analyze the feature importances \n    #(for the best max_depth setting) \n    #(you an refer to the video and demo code)@ 9:37. \n    # Which were the 10 most important pixels for the classifier. \n    # Were the important pixels meaningful to you? \n    \n\n### Max Depth 1 ###\n\nclf = DecisionTreeClassifier(max_depth=1,criterion=\"entropy\").fit(x, y)\nmd1_f = clf.feature_importances_\n\n### Max Depth 2 ###\n\nclf = DecisionTreeClassifier(max_depth=2,criterion=\"entropy\").fit(x, y)\nmd2_f = clf.feature_importances_\n\n\n### Max Depth 3 ###\n\nclf = DecisionTreeClassifier(max_depth=3,criterion=\"entropy\").fit(x, y)\nmd3_f = clf.feature_importances_\n\n### Max Depth 4 ###\n\nclf = DecisionTreeClassifier(max_depth=4,criterion=\"entropy\").fit(x, y)\nmd4_f = clf.feature_importances_\n\n### Max Depth 5 ###\n\nclf = DecisionTreeClassifier(max_depth=5,criterion=\"entropy\").fit(x, y)\nmd5_f = clf.feature_importances_\n\n### Max Depth 6 ###\n\nclf = DecisionTreeClassifier(max_depth=6,criterion=\"entropy\").fit(x, y)\nmd6_f = clf.feature_importances_\n\n### Max Depth 7 ###\n\nclf = DecisionTreeClassifier(max_depth=7,criterion=\"entropy\").fit(x, y)\nmd7_f = clf.feature_importances_\n\n### Max Depth 8 ###\n\nclf = DecisionTreeClassifier(max_depth=8,criterion=\"entropy\").fit(x, y)\nmd8_f = clf.feature_importances_\n\n### Max Depth 9 ###\n\nclf = DecisionTreeClassifier(max_depth=9,criterion=\"entropy\").fit(x, y)\nmd9_f = clf.feature_importances_\n\ntable_f = np.array([md1_f, md2_f, md3_f, md4_f, md5_f, md6_f, md7_f, md8_f, md9_f])\n\nprint(md3_f)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How does the averagecross validation error change for each of these depths? \n       The smaller the depth, the smaller the accuracy and vice versa.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import plot_tree\nfrom matplotlib import pyplot as plt\n\nplt.figure()\nclf = DecisionTreeClassifier(max_depth=1,criterion=\"entropy\").fit(x, y)\nplot_tree(clf,filled=True)\nplt.title('decision tree')\nplt.show()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Suggest some steps you could take that could improve the performance of the classifier.\n\nTo help improve perfromance I would do a 10 K-fold beacuse the lower the k fold the higher risk of bias\n\nFind more people to share how they write numbers to add to the data set, it is a small sample based on the amout of classifiers. \n\n\n\n","metadata":{},"execution_count":null,"outputs":[]}]}